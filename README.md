# DeepLearning_SelfProjects

## Project 1 - Word2Vec
Word2Vec : Word2Vec is a neural network-based model used to generate word embeddings, capturing semantic relationships between words by learning from text data. One of its popular training approaches is the Skip-Gram model, which predicts surrounding words (context) given a target word. It works by sliding a window over a text corpus and forming pairs of target words with their nearby words. The goal is to train a model to maximize the probability of correctly predicting context words based on a given target word. The resulting embeddings encode words as dense vectors in a lower-dimensional space, where semantically similar words appear closer to each other.
